---
layout: '[default_layout]'   
title: 动手学深度学习[个人版]    
date: 2018-09-25 10:47:41  
updated: 
permalink: 
render_drafts: true
copyright: true
password: 
comments: true
toc: true                  
tags:                        
- ML
- DeepLearning
- MXNet
- Glunon

categories:                  
- ML

---
# 1、学习深度须知
## DL???
- 纸上得来终觉浅，绝知此事要躬行。
- 通俗来说，机器学习是一门讨论各式各样适用于不同问题的函数形式，以及如何使用数据来有效地获取函数参数具体值的学科。深度学习是指机器学习中的一类函数，它们的形式通常为多层神经网络。
- 裁剪平均值的雏形：去掉最高分和最低分。
- 绝大多数神经网络的核心原则：
	- 交替使用线性与非线性处理单元，经常被称为“层”。
	- 使用链式法则（即反向传播）来更新网络的参数。
<!--more-->

## MXNet???
- MXNet 是一个开源的**深度学习框架**。它是 AWS（亚马逊云计算服务）首选的深度学习框架，并且也许是最优秀的库【百度百科评价】。
- 对 Python 的支持只是其冰山一角—MXNet 同样提供了对 R、Julia、C++、Scala、Matlab，和 Javascript 的接口。
- MXNet 的ndarray、autograd、gluon等模块的基础功能，所以gluon仅仅是一个模块，而不是框架。

## GPU【图像处理器】和CPU【中央处理器】的区别
- CPU擅长逻辑控制和通用类型数据运算
- GPU擅长的是大规模并发计算

深度学习训练通常需要大量的计算资源。GPU 目前是深度学习最常使用的计算加速硬件。相对于 CPU 来说，GPU 更便宜且计算更加密集。一方面，相同计算能力的 GPU 的价格一般是 CPU 价格的十分之一。另一方面，一台服务器通常可以搭载 8 块或者 16 块 GPU。因此，GPU 数量可以看作是衡量一台服务器的深度学习计算能力的一个标准。

目前独立 GPU 主要有 AMD 和 Nvidia 两家厂商。其中 Nvidia 在深度学习布局较早，对深度学习框架支持更好。因此，目前大家主要会选择 Nvidia 的 GPU。


# 2、数据操作
## 安装
[获取代码并安装运行环境](http://zh.gluon.ai/chapter_prerequisite/install.html)
conda config --prepend channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
下载存储地址：'D:\Program Files\Anaconda\envs\gluon'

小问题：我使用anaconda安装的，首先关于python和Linux的安装都需要先升级工具为最新版本。然而gluon模块安装完成后会有jupyter notebook版本，点击后闪退无果。
正确的打开方式：打开cmd=》激活gluon（activate gluon）=》jupyter notebook

还可以直接安装pip install mxnet.

## 使用
NDArray 和 NumPy 的多维数组非常类似。然而，NDArray 提供 GPU 计算和自动求梯度等更多功能，这些使得 NDArray 更加适合深度学习。

[mxnet的简单使用](http://nbviewer.jupyter.org/github/HanKin2015/Machine_to_DeepingLearning/blob/master/Ipynb/MXNet1.ipynb)

# 3、深度学习基础
单层神经网络：线性回归和 Softmax 回归。

## 线性回归
### 模型
权重weight、偏差bias、参数parameter。
预测（估计）

### 模型训练三要素
训练数据：特征feature、标签label、样本sample、训练集
损失函数：平方误差函数也称平方损失（记得要乘以1/2，其中常数 1/2 使得对平方项求导后的常数系数为 1），然后求和后再取平均值。
优化算法：在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。

当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫做解析解（analytical solution）。
只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫做数值解（numerical solution）。

批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫做超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到合适的超参数。少数情况下，超参数也可以通过模型训练学出。

这个网站主要还是讲的是深度学习，还是先入门机器学习为好。


