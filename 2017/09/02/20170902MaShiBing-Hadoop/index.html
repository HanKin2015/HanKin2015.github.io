<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>马士兵大数据应用 | HanKin的博客 | 聪明出于勤奋，天才在于积累。</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="大数据,hadoop,spark,yarn,map/reduce">
    <meta name="description" content="入门第一课 虚拟机搭建和安装hadoop及启动大数据生态系统1. 存储 Hadoop hdfs  2. 计算引擎 map/reduce v1 map/reduce v2 (map/reduce on yarn) Tez spark">
<meta name="keywords" content="大数据,hadoop,spark,yarn,map&#x2F;reduce">
<meta property="og:type" content="article">
<meta property="og:title" content="马士兵大数据应用">
<meta property="og:url" content="https://hankin2015.github.io/2017/09/02/20170902MaShiBing-Hadoop/index.html">
<meta property="og:site_name" content="HanKin的博客">
<meta property="og:description" content="入门第一课 虚拟机搭建和安装hadoop及启动大数据生态系统1. 存储 Hadoop hdfs  2. 计算引擎 map/reduce v1 map/reduce v2 (map/reduce on yarn) Tez spark">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://www.z4a.net/images/2017/08/28/Hadoop.png">
<meta property="og:updated_time" content="2018-03-17T08:39:28.298Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="马士兵大数据应用">
<meta name="twitter:description" content="入门第一课 虚拟机搭建和安装hadoop及启动大数据生态系统1. 存储 Hadoop hdfs  2. 计算引擎 map/reduce v1 map/reduce v2 (map/reduce on yarn) Tez spark">
<meta name="twitter:image" content="https://www.z4a.net/images/2017/08/28/Hadoop.png">
    
        <link rel="alternate" type="application/atom+xml" title="HanKin的博客" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon-32x32.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">HanKin</h5>
          <a href="mailto:1058198502@qq.com" title="1058198502@qq.com" class="mail">1058198502@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                分类
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/hankin2015" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://hankin2017.coding.me/"  >
                <i class="icon icon-lg icon-link"></i>
                Coding博客
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://www.zhihu.com/people/he-jian-81/activities" target="_blank" >
                <i class="icon icon-lg icon-知乎"></i>
                知乎
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://weibo.com/1846766142/" target="_blank" >
                <i class="icon icon-lg icon-weibo"></i>
                Weibo
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="http://blog.sina.com.cn/s/articlelist_1846766142_0_1.html" target="_blank" >
                <i class="icon icon-lg icon-新浪博客"></i>
                新浪博客
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://blog.csdn.net/Han_kin" target="_blank" >
                <i class="icon icon-lg icon-CSDN"></i>
                CSDN
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://www.cnblogs.com/hankin2017/" target="_blank" >
                <i class="icon icon-lg icon-博客园"></i>
                博客园
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">马士兵大数据应用</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">马士兵大数据应用</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-09-02T15:58:41.000Z" itemprop="datePublished" class="page-time">
  2017-09-02
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/ML/">ML</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#入门第一课-虚拟机搭建和安装hadoop及启动"><span class="post-toc-number">1.</span> <span class="post-toc-text">入门第一课 虚拟机搭建和安装hadoop及启动</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#大数据生态系统"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">大数据生态系统</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-存储"><span class="post-toc-number">1.1.1.</span> <span class="post-toc-text">1. 存储</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-计算引擎"><span class="post-toc-number">1.1.2.</span> <span class="post-toc-text">2. 计算引擎</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-Impala-Presto-Drill-直接跑在hdfs上"><span class="post-toc-number">1.1.3.</span> <span class="post-toc-text">3. Impala Presto Drill 直接跑在hdfs上</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-流式计算-storm"><span class="post-toc-number">1.1.4.</span> <span class="post-toc-text">4. 流式计算 - storm</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#5-kv-store"><span class="post-toc-number">1.1.5.</span> <span class="post-toc-text">5. kv store</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#6-Tensorflow-Mahout"><span class="post-toc-number">1.1.6.</span> <span class="post-toc-text">6. Tensorflow Mahout</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#7-Zookeeper-Protobuf"><span class="post-toc-number">1.1.7.</span> <span class="post-toc-text">7. Zookeeper Protobuf</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#8-sqoop-kafka-flume……"><span class="post-toc-number">1.1.8.</span> <span class="post-toc-text">8. sqoop kafka flume……</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#五个软件"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">五个软件</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#虚拟机安装"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">虚拟机安装</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#入门第二课-hdfs集群集中管理和hadoop文件操作"><span class="post-toc-number">2.</span> <span class="post-toc-text">入门第二课 hdfs集群集中管理和hadoop文件操作</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#入门第三课-java访问hdfs"><span class="post-toc-number">3.</span> <span class="post-toc-text">入门第三课 java访问hdfs</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#入门第四课-Yarn和Map-Reduce配置启动和原理讲解"><span class="post-toc-number">4.</span> <span class="post-toc-text">入门第四课 Yarn和Map/Reduce配置启动和原理讲解</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#入门第五课-java编写mapreduce程序"><span class="post-toc-number">5.</span> <span class="post-toc-text">入门第五课 java编写mapreduce程序</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Hive入门"><span class="post-toc-number">6.</span> <span class="post-toc-text">Hive入门</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Spark"><span class="post-toc-number">7.</span> <span class="post-toc-text">Spark</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#下载"><span class="post-toc-number">7.0.1.</span> <span class="post-toc-text">下载</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#安装"><span class="post-toc-number">7.0.2.</span> <span class="post-toc-text">安装</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#本地运行模式"><span class="post-toc-number">7.0.3.</span> <span class="post-toc-text">本地运行模式</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#zookeeper"><span class="post-toc-number">8.</span> <span class="post-toc-text">zookeeper</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#讲啥"><span class="post-toc-number">8.1.</span> <span class="post-toc-text">讲啥</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#怎么讲"><span class="post-toc-number">8.2.</span> <span class="post-toc-text">怎么讲</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="[default_layout]-20170902MaShiBing-Hadoop"
  class="post-article article-type-[default_layout] fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">马士兵大数据应用</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-09-02 23:58:41" datetime="2017-09-02T15:58:41.000Z"  itemprop="datePublished">2017-09-02</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/ML/">ML</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="入门第一课-虚拟机搭建和安装hadoop及启动"><a href="#入门第一课-虚拟机搭建和安装hadoop及启动" class="headerlink" title="入门第一课 虚拟机搭建和安装hadoop及启动"></a>入门第一课 虚拟机搭建和安装hadoop及启动</h1><h2 id="大数据生态系统"><a href="#大数据生态系统" class="headerlink" title="大数据生态系统"></a>大数据生态系统</h2><h3 id="1-存储"><a href="#1-存储" class="headerlink" title="1. 存储"></a>1. 存储</h3><ul>
<li>Hadoop hdfs</li>
</ul>
<h3 id="2-计算引擎"><a href="#2-计算引擎" class="headerlink" title="2. 计算引擎"></a>2. 计算引擎</h3><ul>
<li>map/reduce v1</li>
<li>map/reduce v2 (map/reduce on yarn)</li>
<li>Tez</li>
<li>spark</li>
</ul>
<a id="more"></a>
<h3 id="3-Impala-Presto-Drill-直接跑在hdfs上"><a href="#3-Impala-Presto-Drill-直接跑在hdfs上" class="headerlink" title="3. Impala Presto Drill 直接跑在hdfs上"></a>3. Impala Presto Drill 直接跑在hdfs上</h3><p>pig(脚本方式) hive(SQL语言) 泡在map/reduce上<br>hive on Tez/sparkSQL</p>
<h3 id="4-流式计算-storm"><a href="#4-流式计算-storm" class="headerlink" title="4. 流式计算 - storm"></a>4. 流式计算 - storm</h3><h3 id="5-kv-store"><a href="#5-kv-store" class="headerlink" title="5. kv store"></a>5. kv store</h3><p>Cassandra mongodb hbase</p>
<h3 id="6-Tensorflow-Mahout"><a href="#6-Tensorflow-Mahout" class="headerlink" title="6. Tensorflow Mahout"></a>6. Tensorflow Mahout</h3><h3 id="7-Zookeeper-Protobuf"><a href="#7-Zookeeper-Protobuf" class="headerlink" title="7. Zookeeper Protobuf"></a>7. Zookeeper Protobuf</h3><h3 id="8-sqoop-kafka-flume……"><a href="#8-sqoop-kafka-flume……" class="headerlink" title="8. sqoop kafka flume……"></a>8. sqoop kafka flume……</h3><h2 id="五个软件"><a href="#五个软件" class="headerlink" title="五个软件"></a>五个软件</h2><ul>
<li>virtualbox</li>
<li>centos.org</li>
<li>hadoop</li>
<li>jdk</li>
<li>xshell(xftp)</li>
<li>pan.baidu.com/s/1i5jzxVR</li>
</ul>
<h2 id="虚拟机安装"><a href="#虚拟机安装" class="headerlink" title="虚拟机安装"></a>虚拟机安装</h2><ul>
<li>VirtualBox</li>
<li>centOS 7</li>
<li>网络设置ipconfig</li>
</ul>
<ol>
<li><p>特别需要注意的地方：<br>将虚拟机的网络设置为host-only,我因为忘了设置成host-only，导致新建的虚拟机和宿主机怎么都ping不通，浪费了我一些时间。<br>选中虚拟机–&gt;设置–&gt;网络</p>
</li>
<li><p>vim /etc/sysconfig/network<br> NETWORKING=yes<br> GATEWAY=192.168.56.1</p>
</li>
<li>vim /etc/sysconfig/network-sripts/ifcfg-enp0s3<br> TYPE=Ethernet<br> IPADDR=192.168.56.100<br> NETMASK=255.255.255.0</li>
<li>修改主机名hostnamectl set-hostname master (主机名千万不能有下划线！)</li>
<li>如果需要虚拟机上网还需要配置/etc/resolv.conf</li>
<li>重启网络service network restart<br>systemctl restart network</li>
<li>检查ssh服务的状态 service network restart</li>
<li>互相ping，看是否测试成功，若不成功，注意防火墙的影响。关闭windows或虚拟机的防火墙。systemctl stop firewalld system disable firewalld<br>win10:192.168.56.1<br>Linux:192.168.56.100</li>
</ol>
<ul>
<li>使用XShell登陆</li>
<li>下载安装Xftp</li>
</ul>
<ol>
<li>检查ssh服务状态systemctl status sshd (service sshd status)，验证使用XShell是否能登陆成功。</li>
<li>将hadoop和jdk上传到虚拟机</li>
<li><p>进入文件夹，安装JDK<br>rpm -ivh ./xxxxx.jdk，验证  i=install v=verble h=打出#号<br>rpm -qa | grep jdk，在命令行中敲java命令，确认jdk已经安装完成<br>jdk默认安装在/usr/java目录下</p>
</li>
<li><p>安装hadoop，解压缩和配置<br> cd /usr/local<br> tar –xvf ./hadoop-2.7.2.tar.gz<br> 把目录修改为hadoop mv hadoop-2… hadoop</p>
<ol>
<li>告诉Hadoop JDK的目录，so修改hadoop-env.sh<br>vi /usr/local/hadoop/etc/hadoop/hadoop-env.sh<br>修改export JAVA_HOME 语句为 export JAVA_HOME=/usr/java/default</li>
<li>把/usr/hadoop/bin和/usr/hadoop/sbin设到PATH中<br>vi /etc/profile<br>追加 export PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin</li>
<li>执行修改的文件 source etc/profile</li>
<li><p>测试hadoop命令是否可以直接执行，任意目录下敲hadoop</p>
<p>cd h<em>  </em>代表了省略</p>
</li>
</ol>
</li>
<li><p>关闭虚拟机shutdown -h now,复制3份</p>
</li>
<li><p>Xshell发送键到所有会话，分别修改虚拟机的ip和hostname，确认互相能够ping通，用ssh登陆，同时修改所有虚拟机的/etc/hosts，确认使用名字可以ping通<br>关闭各个机器上的防火墙 (master / slaves)<br>systemctl stop firewalld<br>systemctl disable firewalld</p>
</li>
<li><p>修改Hadoop配置文件启动 /usr/local/hadoop/etc/hadoop/core-site.xml，指明namenode的信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>ctrl+c退出<br>clear清屏<br>Tab补全</p>
<ol>
<li>修改4台机器的/etc/hosts,让他们通过名字认识对方，测试一下互相用名字可以ping通。<br> 192.168.56.100 master<br> 192.168.56.101 slave1<br> 192.168.56.102 slave2<br> 192.168.56.103 slave3</li>
<li><p>修改master下的/usr/local/hadoop/etc/hadoop/slaves<br> slave1<br> slave2<br> slave3<br>这样，master就可以知道slave1,2,3对应的IP了。</p>
</li>
<li><p>启动namenode和datanode</p>
</li>
</ol>
<p>master上需要格式化namenode，执行指令：</p>
<blockquote>
<p>hadoop namenode -format</p>
</blockquote>
<p>启动master上的namenode，在master上执行：</p>
<blockquote>
<p>hadoop-daemon.sh start namonode</p>
</blockquote>
<p>启动slave上的datanode，在每个slave上执行：</p>
<blockquote>
<p>hadoop-daemon.sh start datanode</p>
</blockquote>
<ol>
<li>使用jps查看namenode和datanode的启动情况。<br>至此，一个master，三个slave的hadoop集群搭建完成并启动成功。<br>业务需求变更永无休止，技术前进就永无止境!<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.z4a.net/images/2017/08/28/Hadoop.png" alt="Hadoop.png" title="">
                </div>
                <div class="image-caption">Hadoop.png</div>
            </figure>
</li>
</ol>
<h1 id="入门第二课-hdfs集群集中管理和hadoop文件操作"><a href="#入门第二课-hdfs集群集中管理和hadoop文件操作" class="headerlink" title="入门第二课 hdfs集群集中管理和hadoop文件操作"></a>入门第二课 hdfs集群集中管理和hadoop文件操作</h1><p>namenode存储文件系统元数据（文件目录结构、分块情况、每款位置、权限等）存在内存中<br>分块存储，备份两份，一般默认128M，所以Hadoop适合大文件。<br>如果要查IP地址，就在开始，运行中输入cmd，回车，输入“ipconfig /all”，再回车，就可以看到。</p>
<p>（1）观察集群配置情况<br>    [root@master ~]# hdfs dfsadmin -report<br>（2）web界面观察集群运行情况<br>使用netstat命令查看端口监听<br>    [root@master ~]# netstat -ntlp<br>浏览器地址栏输入：<a href="http://192.168.56.100:50070" target="_blank" rel="noopener">http://192.168.56.100:50070</a><br>（3）对集群进行集中管理<br>a) 修改master上的/usr/local/hadoop/etc/hadoop/slaves文件<br>    [root@master hadoop]# vim slaves </p>
<pre><code>#编辑内容如下
slave1
slave2
slave3
先使用hadoop-daemon.sh stop namenode（datanode）手工关闭集群。
</code></pre><p>b) 使用start-dfs.sh启动集群<br>    [root@master hadoop]# start-dfs.sh<br>发现需要输入每个节点的密码，太过于繁琐，于是需要配置免密ssh远程登陆。<br>在master上用ssh连接一台slave，需要输入密码slave的密码，<br>    [root@master hadoop]# ssh slave1<br>需要输入密码，输入密码登陆成功后，使用exit指令退回到master。<br>c) 免密ssh远程登陆<br>生成rsa算法的公钥和私钥<br>    [root@master hadoop]# ssh-keygen -t rsa （然后四个回车）<br>进入到/root/.ssh文件夹，可看到生成了id_rsa和id_rsa.pub两个文件。<br>使用以下指令完成免密ssh登陆，拷贝公钥到每个服务器，包括本身<br>    [root@master hadoop]# ssh-copy-id slaveX<br>使用stop-dfs.sh停止集群，然后使用start-dfs.sh启动集群。<br>    [root@master ~]# stop-dfs.sh<br>    [root@master ~]# start-dfs.sh<br>（3）修改windows上的hosts文件，通过名字来访问集群web界面</p>
<p>编辑C:\Windows\System32\drivers\etc\hosts</p>
<p>192.168.56.100 master<br>然后就可以使用<a href="http://master:50070代替http://192.168.56.100:50070" target="_blank" rel="noopener">http://master:50070代替http://192.168.56.100:50070</a></p>
<p>（4） 使用hdfs dfs 或者 hadoop fs命令对文件进行增删改查的操作<br>    1 hadoop fs -ls /<br>    2 hadoop fs -put file /<br>    3 hadoop fs -mkdir /dirname<br>    4 hadoop fs -text /filename<br>    5 hadoop fs -rm /filename<br>将hadoop的安装文件put到了hadoop上操作如下<br>    [root@master local]# hadoop -fs put ./hadoop-2.7.3.tar.gz /<br>通过网页观察文件情况<br>（5）将dfs-site.xml的replication值设为2<br>replication参数是分块拷贝份数，hadoop默认为3。<br>也就是说，一块数据会至少在3台slave上都存在，假如slave节点超过3台了。<br>/usr/local/hadoop/etc/hadoop<br>vim hdfs-site.xml</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;  
    &lt;value&gt;2&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><p>配置文件查询：安装包hadoop-2.7.3\hadoop-2.7.3\share\doc\hadoop\index.html<br>core-default.html<br>hdfs-default.html<br>mapred-default.html<br>yarn-default.html</p>
<p>为了方便测试，同时需要修改另外一个参数dfs.namenode.heartbeat.recheck-interval，这个值默认为300s，<br>将其修改成10000，单位是ms，这个参数是定期间隔时间后检查slave的运行情况并更新slave的状态。<br>可以通过 hadoop-2.7.3\share\doc\hadoop\index.html里面查找这些默认的属性</p>
<p>修改完hdfs-site.xml文件后，重启hadoop集群，<br>stop-dfs.sh  #停止hadoop集群<br>start-dfs.sh #启动hadoop集权<br>hadoop -fs put ./jdk-8u91-linux-x64.rpm / #将jdk安装包上传到hadoop的根目录<br>到web页面上去观察jdk安装包文件分块在slave1，slave2，slave3的存储情况<br>hadoop-daemon.sh stop datanode #在slave3上停掉datanode<br>等一会时间后（大概10s，前面修改了扫描slave运行情况的间隔时间为10s），刷新web页面<br>观察到slave3节点挂掉<br>hadoop-daemon.sh start datanode #在slave3上启动datanode<br>然后再去观察jdk安装包文件分块在slave1，slave2，slave3的存储情况</p>
<h1 id="入门第三课-java访问hdfs"><a href="#入门第三课-java访问hdfs" class="headerlink" title="入门第三课 java访问hdfs"></a>入门第三课 java访问hdfs</h1><p>(1)关于hdfs小结<br>hadoop由hdfs + yarn + map/reduce组成，<br>hdfs是数据库存储模块，主要由1台namenode和n台datanode组成的一个集群系统，<br>datanode可以动态扩展，文件根据固定大小分块（默认为128M），<br>每一块数据默认存储到3台datanode，故意冗余存储，防止某一台datanode挂掉，数据不会丢失。<br>HDFS = NameNode + SecondaryNameNode + journalNode + DataNode<br>hdfs的典型应用就是：百度云盘</p>
<p>（2）修改hadoop.tmp.dir默认值<br>hadoop.tmp.dir默认值为/tmp/hadoop-${user.name}，由于/tmp目录是系统重启时候会被删除，所以应该修改目录位置。<br>修改core-site.xml（在所有节点上都修改）<br>    [root@master ~]#  vim core-site.xml<br>    <property><br>    <name>hadoop.tmp.dir</name><br>    <value>/var/hadoop</value><br>    </property><br>修改完namenode和datanode上的hadoop.tmp.dir参数后，需要格式化namenode，在master上执行：<br>    [root@master ~]# hdfs namenode -format<br>    [root@master ~]# stop-dfs.sh<br>    [root@master ~]# start-dfs.sh<br>    hadoop-daemon.sh start datanode<br>通过java程序访问hdfs，就把HDFS集群当成一个大的系统磁盘就行了！</p>
<p>安装myeclipse，破解，导入jar包，在master中创建文件，上传到/目录下。</p>
<p>windows上的权限系统和linux上的权限系统，测试期间为了简单起见可以关闭权限检查 在namenode的hdfs-site.xml上，添加配置：<br>    <property><br>        <name>dfs.permissions.enabled</name><br>        <value>false</value><br>    </property><br>重新启动namenode:<br>    [root@master ~]# hadoop-daemon.sh stop namenode<br>    [root@master ~]# hadoop-daemon.sh start namenode<br>(5) 使用FileSyste类来读写hdfs</p>
<pre><code>package com.hadoop.hdfs;

import java.io.FileInputStream;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

public class HelloHDFS {

    public static Log log =  LogFactory.getLog(HelloHDFS.class);

    public static void main(String[] args) throws Exception {

        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://192.168.56.100:9000&quot;);
        conf.set(&quot;dfs.replication&quot;, &quot;2&quot;);//默认为3
        FileSystem fileSystem = FileSystem.get(conf);

        boolean success = fileSystem.mkdirs(new Path(&quot;/yucong&quot;));
        log.info(&quot;创建文件是否成功:&quot; + success);

        success = fileSystem.exists(new Path(&quot;/yucong&quot;));
        log.info(&quot;文件是否存在:&quot; + success);

        success = fileSystem.delete(new Path(&quot;/yucong&quot;), true);
        log.info(&quot;删除文件是否成功：&quot; + success);

        /*FSDataOutputStream out = fileSystem.create(new Path(&quot;/test.data&quot;), true);
        FileInputStream fis = new FileInputStream(&quot;c:/test.txt&quot;);
        IOUtils.copyBytes(fis, out, 4096, true);*/

        FSDataOutputStream out = fileSystem.create(new Path(&quot;/test2.data&quot;));
        FileInputStream in = new FileInputStream(&quot;c:/test.txt&quot;);
        byte[] buf = new byte[4096];
        int len = in.read(buf);
        while(len != -1) {
            out.write(buf,0,len);
            len = in.read(buf);
        }
        in.close();
        out.close();

        FileStatus[] statuses = fileSystem.listStatus(new Path(&quot;/&quot;));
        log.info(statuses.length);
        for(FileStatus status : statuses) {
            log.info(status.getPath());
            log.info(status.getPermission());
            log.info(status.getReplication());
        }
    }

}
</code></pre><p>这是一个maven项目，pom.xml文件为：</p>
<pre><code>&lt;dependencies&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
    &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
    &lt;version&gt;2.7.3&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
      &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
     &lt;version&gt;2.7.3&lt;/version&gt;
  &lt;/dependency&gt;

&lt;/dependencies&gt;
</code></pre><hr>
<p>复习：</p>
<ol>
<li>hadoop分为几个逻辑组件？<br> a. hdfs yarn mapred<br> b. hdfs-&gt; namenode datanode secondarynamenode journalnode</li>
<li>hdfs的典型结构？ 主从结构（物理结构+逻辑结构）</li>
<li>hdfs如何实现横向扩展？加一台机器作为datanode，链接到namenode上</li>
<li>hdfs的典型应用设计？360网盘  百度网盘</li>
<li>网盘中“保存到我的网盘”会复制一份数据到你的网盘吗？ 不会，我的网盘作为一个namenode。</li>
</ol>
<hr>
<p>没讲什么：</p>
<ol>
<li>安全与权限 kerberos</li>
<li>secondary namenode  -&gt; check point namenode</li>
<li>HA实现</li>
<li>Federation超大规模数据中心</li>
</ol>
<h1 id="入门第四课-Yarn和Map-Reduce配置启动和原理讲解"><a href="#入门第四课-Yarn和Map-Reduce配置启动和原理讲解" class="headerlink" title="入门第四课 Yarn和Map/Reduce配置启动和原理讲解"></a>入门第四课 Yarn和Map/Reduce配置启动和原理讲解</h1><p>分布式计算<br>设计原则：移动计算，而不是移动数据<br>Apache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的 Hadoop 资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。</p>
<p>Caffe （卷积神经网络框架）<br>Caffe，全称Convolutional Architecture for Fast Feature Embedding。<br>caffe是一个清晰，可读性高，快速的深度学习框架。作者是贾扬清，加州大学伯克利的ph.D，现就职于Facebook。caffe的官网是<a href="http://caffe.berkeleyvision.org/。" target="_blank" rel="noopener">http://caffe.berkeleyvision.org/。</a></p>
<ol>
<li>配置计算调度系统Yarn和计算引擎Map/Reduce，所有机器同时配置。</li>
<li>namenode上配置mapred-site.xml <property><br>     <name>mapreduce.framework.name</name><br>     <value>yarn</value><br> </property></li>
<li><p>yarn-site.xml的配置</p>
 <property><br>     <name>yarn.resourcemanager.hostname</name><br>     <value>master</value><br> </property>

 <property><br>     <name>yarn.nodemanager.aux-services</name><br>     <value>mapreduce_shuffle</value><br> </property>  

 <property><br>     <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name><br>     <value>org.apache.hadoop.mapred.ShuffleHandler</value><br> </property></li>
<li>启动yarn集群start-yarn.sh</li>
<li>jps观察启动结果</li>
<li>可以使用yarn-daemon.sh单独启动resourcemanager和nodemanager</li>
<li>通过网页<a href="http://master:8088/观察yarn集群" target="_blank" rel="noopener">http://master:8088/观察yarn集群</a></li>
<li>实验：建立一个wordcount.txt文件，利用map reduce计算里面单词的数量。<br>hadoop fs -mkdir /input<br>hadoop fs -put wordcount.txt /input/<br>find /usr/local/hadoop -name <em>example</em>.jar 查找示例文件</li>
<li>通过hadoop jar xxx.jar wordcount /input /output来运行示例程序</li>
<li>通过网页来观察该job的运行情况</li>
<li>hadoop job -kill job_id –&gt; mapred job -kill job_id mapred job -list</li>
<li>经验：yarn-site如果是集中启动，其实只需要在管理机上配置一份即可，但是如果单独启动，需要每台机器一份，在网页上可以看到当前机器的配置，以及这个配置的来源</li>
</ol>
<h1 id="入门第五课-java编写mapreduce程序"><a href="#入门第五课-java编写mapreduce程序" class="headerlink" title="入门第五课 java编写mapreduce程序"></a>入门第五课 java编写mapreduce程序</h1><p>以上任何对环境变量的修改，都需要重新启动电脑使配置生效，因此可将所需软件全部安装配置好后再重启电脑。</p>
<ol>
<li>java开发map_reduce程序</li>
<li>配置系统环境变量HADOOP_HOME，指向hadoop安装目录（如果你不想招惹不必要的麻烦，不要在目录中包含空格或者中文字符）<br>把HADOOP_HOME/bin加到PATH环境变量（非必要，只是为了方便）</li>
<li>如果是在windows下开发，需要添加windows的库文件<br> a. 把盘中共享的bin目录覆盖HADOOP_HOME/bin<br> b. 如果还是不行，把其中的hadoop.dll复制到c:\windows\system32目录下，可能需要重启机器</li>
<li>建立新项目，引入hadoop需要的jar文件</li>
<li><p>代码WordMapper：</p>
<p> import java.io.IOException;<br> import org.apache.hadoop.io.IntWritable;<br> import org.apache.hadoop.io.LongWritable;<br> import org.apache.hadoop.io.Text;<br> import org.apache.hadoop.mapreduce.Mapper;</p>
</li>
</ol>
<pre><code>public class WordMapper extends Mapper&lt;LongWritable,Text, Text, IntWritable&gt; {

    @Override
    protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)
            throws IOException, InterruptedException {
        String line = value.toString();
        String[] words = line.split(&quot; &quot;);
        for(String word : words) {
            context.write(new Text(word), new IntWritable(1));
        }
    }

}
</code></pre><ol>
<li><p>代码WordReducer：</p>
<p> import java.io.IOException;<br> import org.apache.hadoop.io.IntWritable;<br> import org.apache.hadoop.io.LongWritable;<br> import org.apache.hadoop.io.Text;<br> import org.apache.hadoop.mapreduce.Reducer;</p>
<p> public class WordReducer extends Reducer<text, intwritable,="" text,="" longwritable=""> {</text,></p>
<pre><code>@Override
protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,
        Reducer&lt;Text, IntWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
    long count = 0;
    for(IntWritable v : values) {
        count += v.get();
    }
    context.write(key, new LongWritable(count));
}
</code></pre><p> }</p>
</li>
<li><p>代码Test：</p>
<p> import org.apache.hadoop.conf.Configuration;<br> import org.apache.hadoop.fs.Path;<br> import org.apache.hadoop.io.IntWritable;<br> import org.apache.hadoop.io.LongWritable;<br> import org.apache.hadoop.io.Text;<br> import org.apache.hadoop.mapreduce.Job;<br> import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br> import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
</li>
</ol>
<pre><code>public class Test {
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();

        Job job = Job.getInstance(conf);

        job.setMapperClass(WordMapper.class);
        job.setReducerClass(WordReducer.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(LongWritable.class);

        FileInputFormat.setInputPaths(job, &quot;c:/bigdata/hadoop/test/test.txt&quot;);
        FileOutputFormat.setOutputPath(job, new Path(&quot;c:/bigdata/hadoop/test/out/&quot;));

        job.waitForCompletion(true);
    }
}
</code></pre><ol>
<li><p>把hdfs中的文件拉到本地来运行</p>
<p> FileInputFormat.setInputPaths(job, “hdfs://master:9000/wcinput/“);<br> FileOutputFormat.setOutputPath(job, new Path(“hdfs://master:9000/wcoutput2/“));<br>注意这里是把hdfs文件拉到本地来运行，如果观察输出的话会观察到jobID带有local字样<br>同时这样的运行方式是不需要yarn的(自己停掉yarn服务做实验)</p>
</li>
<li><p>在远程服务器执行</p>
<p> conf.set(“fs.defaultFS”, “hdfs://master:9000/“);</p>
<p> conf.set(“mapreduce.job.jar”, “target/wc.jar”);<br> conf.set(“mapreduce.framework.name”, “yarn”);<br> conf.set(“yarn.resourcemanager.hostname”, “master”);<br> conf.set(“mapreduce.app-submission.cross-platform”, “true”);</p>
<p> FileInputFormat.setInputPaths(job, “/wcinput/“);<br> FileOutputFormat.setOutputPath(job, new Path(“/wcoutput3/“));<br>如果遇到权限问题，配置执行时的虚拟机参数-DHADOOP_USER_NAME=root</p>
</li>
<li>也可以将hadoop的四个配置文件拿下来放到src根目录下，就不需要进行手工配置了，默认到classpath目录寻找</li>
<li>或者将配置文件放到别的地方，使用conf.addResource(.class.getClassLoader.getResourceAsStream)方式添加，不推荐使用绝对路径的方式</li>
<li><p>建立maven-hadoop项目：</p>
<p><project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemalocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"><br>  <modelversion>4.0.0</modelversion><br>  <groupid>mashibing.com</groupid><br>  <artifactid>maven</artifactid><br>  <version>0.0.1-SNAPSHOT</version><br>  <name>wc</name></project></p>
  <description>hello mp</description>


</li>
</ol>
<pre><code>  &lt;properties&gt;
        &lt;project.build.sourceencoding&gt;UTF-8&lt;/project.build.sourceencoding&gt;
        &lt;hadoop.version&gt;2.7.3&lt;/hadoop.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.12&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
            &lt;version&gt;${hadoop.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
            &lt;version&gt;${hadoop.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
            &lt;version&gt;${hadoop.version}&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;


&lt;/project&gt;
</code></pre><ol>
<li><p>配置log4j.properties，放到src/main/resources目录下</p>
<p>log4j.rootCategory=INFO, stdout</p>
<p>log4j.appender.stdout=org.apache.log4j.ConsoleAppender<br>log4j.appender.stdout.layout=org.apache.log4j.PatternLayout<br>log4j.appender.stdout.layout.ConversionPattern=[QC] %p [%t] %C.%M(%L) | %m%n</p>
</li>
</ol>
<h1 id="Hive入门"><a href="#Hive入门" class="headerlink" title="Hive入门"></a>Hive入门</h1><ol>
<li>Hive入门</li>
<li><p>tar -xvf 解压Hive，到/usr/local目录，将解压后的目录名mv为hive<br>设定环境变量HADOOP_HOME，HIVE_HOME，将bin目录加入到PATH中<br> vim /etc/profile<br> export PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin<br> export HADOOP_HOME=/usr/local/hadoop<br> export HIVE_HOME=/usr/local/hive<br> export PATH=$PATH:$HIVE_HOME/bin</p>
</li>
<li><p>cd /usr/local/hive/conf<br>cp hive-default.xml.template hive-site.xml<br>修改hive.metastore.schema.verification，设定为false<br>创建/usr/local/hive/tmp目录，替换${system:java.io.tmpdir}为该目录<br>替换${system:user.name}为root</p>
</li>
<li><p>schematool -initSchema -dbType derby<br>会在当前目录下简历metastore_db的数据库。<br>注意！！！下次执行hive时应该还在同一目录，默认到当前目录下寻找metastore。<br>遇到问题，把metastore_db删掉，重新执行命令<br>实际工作环境中，经常使用mysql作为metastore的数据</p>
</li>
<li>启动hive</li>
<li>观察hadoop fs -ls /tmp/hive中目录的创建</li>
<li>show databases;<br>use default;<br>create table doc(line string);<br>show tables;<br>desc doc;<br>select * from doc;<br>drop table doc;</li>
<li>观察hadoop fs -ls /user</li>
<li>启动yarn</li>
<li>load data inpath ‘/wcinput’ overwrite into table doc;<br>select <em> from doc;<br>select split(line, ‘ ‘) from doc;<br>select explode(split(line, ‘ ‘)) from doc;<br>select word, count(1) as count from (select explode(split(line, ‘ ‘)) as word from doc) w group by word;<br>select word, count(1) as count from (select explode(split(line, ‘ ‘)) as word from doc) w group by word order by word;<br>create table word_counts as select word, count(1) as count from (select explode(split(line, ‘ ‘)) as word from doc) w group by word order by word;<br>select </em> from word_counts;<br>dfs -ls /user/hive/…</li>
<li>使用sougou搜索日志做实验</li>
<li>将日志文件上传的hdfs系统，启动hive<br>hadoop fs -put sougou.dic /</li>
<li>create table sougou (qtime string, qid string, qword string, url string) row format delimited fields terminated by ‘,’;  划界区域被逗号终止<br>desc sougou; 查看建立的sougou表</li>
<li>load data inpath ‘/sougou.dic’ into table sougou;</li>
<li>select count(*) from sougou;</li>
<li>create table sougou_results as select keyword, count(1) as count from (select qword as keyword from sougou) t group by keyword order by count desc;</li>
<li>select * from sougou_results limit 10;</li>
</ol>
<p>概念<br>用类似SQL语言的方式做计算<br>将SQL语句查询，转换为MapReduce操作，省去Java编程<br>应用场合：<br>静态数据分析：数据不会频繁变化，不需要实时结果响应<br>OLAP OLTP</p>
<h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><p>内存计算引擎<br>使用scala开发<br>支持Java scala python开发接口</p>
<h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p>地址spark.apache.org</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>复制一台单独的虚拟机，名c<br>修改其ip，192.168.56.200<br>修改其hostname为c，hostnamectl set-hostname c<br>修改/etc/hosts加入对本机的解析<br>重启网络服务 systemctl restart network<br>上传spark安装文件到root目录<br>解压spark到/usr/local下，将其名字修改为spark</p>
<h3 id="本地运行模式"><a href="#本地运行模式" class="headerlink" title="本地运行模式"></a>本地运行模式</h3><ol>
<li><p>使用spark-submit提交job<br>cd /usr/local/spark<br>./bin/spark-submit –class org.apache.spark.examples.SparkPi ./examples/jars/spark-examples_2.11-2.1.0.jar 10000</p>
</li>
<li><p>使用spark-shell进行交互式提交<br> 创建root下的文本文件hello.txt<br> ./bin/spark-shell<br> 再次连接一个terminal，用jps观察进程，会看到spark-submit进程<br> sc<br> sc.textFile(“/root/hello.txt”)<br> val lineRDD = sc.textFile(“/root/hello.txt”)<br> lineRDD.foreach(println)<br> 观察网页端情况<br> val wordRDD = lineRDD.flatMap(line =&gt; line.split(“ “))<br> wordRDD.collect<br> val wordCountRDD = wordRDD.map(word =&gt; (word,1))<br> wordCountRDD.collect<br> val resultRDD = wordCountRDD.reduceByKey((x,y)=&gt;x+y)<br> resultRDD.collect<br> val orderedRDD = resultRDD.sortByKey(false)<br> orderedRDD.collect<br> orderedRDD.saveAsTextFile(“/root/result”)<br> 观察结果<br> 简便写法：sc.textFile(“/root/hello.txt”).flatMap(<em>.split(“ “)).map((</em>,1)).reduceByKey(<em>+</em>).sortByKey().collect<br> sc.textFile(“/root/wordcount.txt”).flatMap(<em>.split(“ “)).map((</em>,1)).reduceByKey(<em>+</em>).sortByKey(false).saveAsTextFile(“/root/result”)</p>
</li>
<li><p>使用local模式访问hdfs数据<br> start-dfs.sh<br> spark-shell执行：sc.textFile(“hdfs://192.168.56.100:9000/hello.txt”).flatMap(<em>.split(“ “)).map((</em>,1)).reduceByKey(<em>+</em>).sortByKey().collect （可以把ip换成master，修改/etc/hosts）<br> sc.textFile(“hdfs://192.168.56.100:9000/hello.txt”).flatMap(<em>.split(“ “)).map((</em>,1)).reduceByKey(<em>+</em>).sortByKey().saveAsTextFile(“hdfs://192.168.56.100:9000/output1”)</p>
</li>
<li><p>spark standalone模式<br> 在master和所有slave上解压spark<br> 修改master上conf/slaves文件，加入slave<br> 修改conf/spark-env.sh，export SPARK_MASTER_HOST=master<br> 复制spark-env.sh到每一台slave<br> cd /usr/local/spark<br> ./sbin/start-all.sh<br> 在c上执行：./bin/spark-shell –master spark://192.168.56.100:7077 (也可以使用配置文件)<br> 观察<a href="http://master:8080" target="_blank" rel="noopener">http://master:8080</a></p>
</li>
</ol>
<p>spark on yarn模式<br>spark mesos模式</p>
<h1 id="zookeeper"><a href="#zookeeper" class="headerlink" title="zookeeper"></a>zookeeper</h1><h2 id="讲啥"><a href="#讲啥" class="headerlink" title="讲啥"></a>讲啥</h2><ul>
<li>zookeeper是什么，有什么用</li>
<li>安装、配置、启动、监控</li>
<li>java api</li>
<li>HA的开发</li>
<li>cap理论/paxos算法</li>
</ul>
<h2 id="怎么讲"><a href="#怎么讲" class="headerlink" title="怎么讲"></a>怎么讲</h2><p>大篇理论，很少实践<br>实践中穿插理论学习</p>
<p>一致  有头  数据树<br>Google三论文：GFS、BigTable、MapReduce<br>2、安装配置（注意：集群的所有机器都需要设置）</p>
<p>下载zookeeper-3.4.10.tar.gz<br>解压：tar xvf zookeeper-3.4.10.tar.gz<br>配置　　</p>
<p>切换到/zookeeper/conf目录：/usr/local/zookeeper-3.4.10/conf （我的路径）<br>server.n=xxx.xxx.xx:2888:3888<br>拷贝：cp zoo_sample.cfg zoo.cfg<br>修改zoo.cfg：vim zoo.cfg<br>dataDir=/tmp/zookeeper （数据存储位置，生产环境需要修改，这个是linux的临时目录，可能会被删除）<br>在配置文件底部添加一下内容：我这个配置了域名，如果没有配置域名就用ip。<br>server.1=master:2888:3888<br>server.2=slave2:2888:3888<br>server.3=slave3:2888:3888<br>修改数据文件<br>切换/tmp目录：cd /tmp<br>创建zookeeper目录：mkdir zookeeper<br>切换至zookeeper目录：cd /tmp/zookeeper<br>创建myid文件：vim myid<br>master上，输入1保存；slave2，输入2保存；slave3，输入3保存。<br>启动、观测</p>
<p>切换至/zookeeper/bin目录：/usr/local/zookeeper-3.4.10/bin<br>服务端<br>启动：./zkServer.sh start<br>查看：./zkServer.sh status<br>停止：./zkServer.sh stop<br>jps（查看状态）<br>2289 QuorumPeerMain<br>2302 Jps<br>zookeeper集群建议使用奇数。其中有一个leader，其余是follower。<br>客户端　　.<br>./zkCli.sh -server master:2181<br>create /tank tankservers<br>create /tank/server1 server1info<br>create /tank/server2 server2info<br>create /tank/server3 server3info<br>ls /tank<br>get /tank<br>set /tank tankserversinfo<br>get /tank<br>delete /tank/server3</p>
<p><a href="http://www.cnblogs.com/qiuyong/p/7081893.html" target="_blank" rel="noopener">http://www.cnblogs.com/qiuyong/p/7081893.html</a></p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-03-17T08:39:28.298Z" itemprop="dateUpdated">2018-03-17 16:39:28</time>
</span><br>


        
        本文链接：<a href="/2017/09/02/20170902MaShiBing-Hadoop/" target="_blank" rel="external">https://hankin2015.github.io/2017/09/02/20170902MaShiBing-Hadoop/</a></br>版权声明： 本站所有文章除特别声明外，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！
        
    </div>
    
    <footer>
        <a href="https://hankin2015.github.io">
            <img src="/img/avatar.jpg" alt="HanKin">
            HanKin
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/map-reduce/">map/reduce</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/yarn/">yarn</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://hankin2015.github.io/2017/09/02/20170902MaShiBing-Hadoop/&title=《马士兵大数据应用》 — HanKin的博客&pic=https://hankin2015.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://hankin2015.github.io/2017/09/02/20170902MaShiBing-Hadoop/&title=《马士兵大数据应用》 — HanKin的博客&source=入门第一课 虚拟机搭建和安装hadoop及启动大数据生态系统1. 存储
Hadoop hdfs

2. 计算引擎
map/reduce v1
map/re..." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://hankin2015.github.io/2017/09/02/20170902MaShiBing-Hadoop/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《马士兵大数据应用》 — HanKin的博客&url=https://hankin2015.github.io/2017/09/02/20170902MaShiBing-Hadoop/&via=https://hankin2015.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://hankin2015.github.io/2017/09/02/20170902MaShiBing-Hadoop/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/09/07/20170907File-operation/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">python文件操作</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/09/02/20170902ML-lixin/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">机器学习python应用</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'false' == 'true',
            verify: 'false' == 'true',
            appId: "5PSlFbHUS2S5raMdDvNpy2NQ-gzGzoHsz",
            appKey: "nCO1l5gwuu3sN0FdViygh7Fp",
            avatar: "mm",
            placeholder: "Just go go",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->







</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        赏个糖吃吧~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>HanKin &copy; 2015 - 2019</span>
            <span>
                
                <a href="http://www.miitbeian.gov.cn/" target="_blank">渝ICP备1314520号-1</a><br>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://hankin2015.github.io/2017/09/02/20170902MaShiBing-Hadoop/&title=《马士兵大数据应用》 — HanKin的博客&pic=https://hankin2015.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://hankin2015.github.io/2017/09/02/20170902MaShiBing-Hadoop/&title=《马士兵大数据应用》 — HanKin的博客&source=入门第一课 虚拟机搭建和安装hadoop及启动大数据生态系统1. 存储
Hadoop hdfs

2. 计算引擎
map/reduce v1
map/re..." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://hankin2015.github.io/2017/09/02/20170902MaShiBing-Hadoop/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《马士兵大数据应用》 — HanKin的博客&url=https://hankin2015.github.io/2017/09/02/20170902MaShiBing-Hadoop/&via=https://hankin2015.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://hankin2015.github.io/2017/09/02/20170902MaShiBing-Hadoop/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="//api.qrserver.com/v1/create-qr-code/?data=https://hankin2015.github.io/2017/09/02/20170902MaShiBing-Hadoop/" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


lazyScripts.push('//s95.cnzz.com/z_stat.php?id=1274378743&web_id=1274378743')

</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '死鬼去哪里了！';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂)咦!又好了!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
